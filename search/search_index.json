{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Databook","text":"<p>Learning data science through Practical Statistics for Data Scientists (O\u2019Reilly).</p> <p>This repo includes my notes, code snippets, and small experiments following the book \u2014 all aimed at building a solid stats foundation for real-world data work.</p>"},{"location":"index.html#structure","title":"Structure","text":"<ul> <li><code>src/</code> \u2014 Jupyter notebooks for coding parts of the book</li> <li><code>src/data/</code> \u2014 datasets used in examples (sources below)</li> <li><code>src/notes/</code> \u2014 my notes as PDFs and <code>rnote</code> files</li> </ul>"},{"location":"index.html#see-also","title":"See also","text":"<ul> <li> Practical Statistics for Data Scientists GitHub Repo </li> <li> mkdocs-jupyter </li> <li> mkdocs-material </li> </ul>"},{"location":"00_movie_analysis.html","title":"Intro: IMDb Analysis","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import matplotlib.pyplot as plt <p>We create a DataFrame using the iMDB data we found on Kaggle.</p> In\u00a0[2]: Copied! <pre>df = pd.read_csv(\"./data/imdb.csv\")\n</pre> df = pd.read_csv(\"./data/imdb.csv\") <p>Some columns are not processable since they are string values, so we convert them into numerical data.</p> In\u00a0[3]: Copied! <pre>df[\"duration\"] = df[\"duration\"].str.replace(\" min\", \"\", regex=False).astype(float)\ndf[\"year_start\"] = df[\"year\"].str.extract(r\"(\\d{4})\").fillna(0).astype(int)\n</pre> df[\"duration\"] = df[\"duration\"].str.replace(\" min\", \"\", regex=False).astype(float) df[\"year_start\"] = df[\"year\"].str.extract(r\"(\\d{4})\").fillna(0).astype(int) <p>Also, we create a new column called <code>decade</code> so that we will be able to use it for grouping purposes.</p> In\u00a0[4]: Copied! <pre>df = df[~df[\"year_start\"].isin([0])]\ndf[\"decade\"] = (df[\"year_start\"] // 10) * 10\n</pre> df = df[~df[\"year_start\"].isin([0])] df[\"decade\"] = (df[\"year_start\"] // 10) * 10 <p>Now, just to have an image in mind, we create a line plot to see the correlation between movies and the decade they were released.</p> In\u00a0[5]: Copied! <pre>year_avg = df.groupby(\"decade\")[\"rating\"].mean().reset_index()\nyear_rating_fig, year_rating_ax = plt.subplots(figsize=[6, 4])\nyear_rating_ax.plot(year_avg[\"decade\"], year_avg[\"rating\"])\nplt.show()\n</pre> year_avg = df.groupby(\"decade\")[\"rating\"].mean().reset_index() year_rating_fig, year_rating_ax = plt.subplots(figsize=[6, 4]) year_rating_ax.plot(year_avg[\"decade\"], year_avg[\"rating\"]) plt.show() <p>To analyze more, we check out what column have what kind of value.</p> In\u00a0[6]: Copied! <pre>print(df.dtypes)\n</pre> print(df.dtypes) <pre>title           object\nyear            object\ncertificate     object\nduration       float64\ngenre           object\nrating         float64\ndescription     object\nstars           object\nvotes           object\nyear_start       int64\ndecade           int64\ndtype: object\n</pre> <p>Okay, so we can see that we still have a lot of objects, let's see if we can turn them into numerical or categorical values.</p> In\u00a0[7]: Copied! <pre>df[\"votes\"]\n</pre> df[\"votes\"] Out[7]: <pre>0       177,031\n1       199,885\n2       501,384\n3         9,773\n4        15,413\n         ...   \n9952      3,130\n9953    970,067\n9954    199,898\n9955    439,601\n9956      9,786\nName: votes, Length: 9326, dtype: object</pre> <p>We can turn the entire <code>votes</code> column to integers.</p> In\u00a0[8]: Copied! <pre>df[\"votes_num\"] = df[\"votes\"].str.replace(\",\", \"\").fillna(0).astype(int)\nprint(f\"{df[\"votes_num\"]}\\n\")\nprint(df.dtypes)\n</pre> df[\"votes_num\"] = df[\"votes\"].str.replace(\",\", \"\").fillna(0).astype(int) print(f\"{df[\"votes_num\"]}\\n\") print(df.dtypes) <pre>0       177031\n1       199885\n2       501384\n3         9773\n4        15413\n         ...  \n9952      3130\n9953    970067\n9954    199898\n9955    439601\n9956      9786\nName: votes_num, Length: 9326, dtype: int64\n\ntitle           object\nyear            object\ncertificate     object\nduration       float64\ngenre           object\nrating         float64\ndescription     object\nstars           object\nvotes           object\nyear_start       int64\ndecade           int64\nvotes_num        int64\ndtype: object\n</pre> <p>Let's see if popularity and the iMDB rating is correlated.</p> In\u00a0[9]: Copied! <pre>df[['rating', \"votes_num\"]].corr()\n</pre> df[['rating', \"votes_num\"]].corr() Out[9]: rating votes_num rating 1.000000 0.142622 votes_num 0.142622 1.000000 <p>So we can roughly say that people will share their opinion even if the movie was great or not. Though we will share our opinion a tiny bit more if the movies was great.</p> In\u00a0[10]: Copied! <pre>df.sort_values([\"rating\", \"votes\"], ascending=[False, False], inplace=True)\ndf\n</pre> df.sort_values([\"rating\", \"votes\"], ascending=[False, False], inplace=True) df Out[10]: title year certificate duration genre rating description stars votes year_start decade votes_num 9444 BoJack Horseman (2014\u20132020) TV-MA 26.0 Animation, Comedy, Drama 9.9 BoJack reconnects with faces from his past. ['Amy Winfrey', '| ', '    Stars:', 'Will Arne... 16,066 2014 2010 16066 17 1899 (2022\u2013 ) NaN 60.0 Drama, History, Horror 9.6 Multinational immigrants traveling from the ol... ['Ben Ashenden, ', 'Aneurin Barnard, ', 'Emily... 853 2022 2020 853 8161 Avatar: The Last Airbender (2005\u20132008) TV-Y7 25.0 Animation, Action, Adventure 9.6 The heroes work together to stop Azula's destr... ['Michael Dante DiMartino', '| ', '    Stars:'... 5,221 2005 2000 5221 9578 Stranger Things (2016\u2013 ) TV-14 98.0 Drama, Fantasy, Horror 9.6 As Hopper braces to battle a monster, Dustin d... ['Matt Duffer, ', 'Ross Duffer', '| ', '    St... 36,276 2016 2010 36276 8907 Avatar: The Last Airbender (2005\u20132008) TV-Y7 24.0 Animation, Action, Adventure 9.6 As the Fire Nation continues its assault on th... ['Dave Filoni', '| ', '    Stars:', 'Zach Tyle... 3,953 2005 2000 3953 ... ... ... ... ... ... ... ... ... ... ... ... ... 9752 Hey Duggee (2014\u2013 ) TV-Y 8.0 Animation, Comedy, Family NaN Duggee and the Squirrels bake a pie and learn ... ['Adam Longworth'] NaN 2014 2010 0 9802 Hey Duggee (2014\u2013 ) TV-Y 8.0 Animation, Comedy, Family NaN Duggee arranges for the Squirrels to write to ... ['Adam Longworth, ', 'Lily-Summer Williams'] NaN 2014 2010 0 9813 Oni: Thunder God's Tale (2022\u2013 ) TV-Y7 NaN Animation, Action, Adventure NaN Add a Plot [\"Daisuke 'Dice' Tsutsumi\", '| ', '    Stars:'... NaN 2022 2020 0 9842 Hollywood Insider (2018\u2013 ) NaN NaN Talk-Show NaN Behind the scenes of The Irishman. ['Bobby Cannavale, ', 'Robert De Niro, ', 'Al ... NaN 2018 2010 0 9843 Hollywood Insider (2018\u2013 ) NaN NaN Talk-Show NaN Reactions at premiere of The Irishman. ['Pritan Ambroase', '| ', '    Stars:', 'Rober... NaN 2018 2010 0 <p>9326 rows \u00d7 12 columns</p> <p>We can see that movies with very low votes also pops up in our Top 10 chart. We have to solve with Bayesian average forumla.</p> <p>$$ \\begin{aligned} \\text{Bayesian Average} = \\frac{C\\cdot m + v \\cdot R}{C+v} \\\\ \\text{R = average rating of the movie} \\\\ \\text{v = total amount of votes for the movie} \\\\ \\text{m = average rating of all the movies} \\\\ \\text{C = average vote count of all movies} \\end{aligned} $$</p> In\u00a0[11]: Copied! <pre>m = df['rating'].mean()\nc = df[\"votes_num\"].mean()\nprint(m)\nprint(c)\ndf[\"bayes_rating\"] = ((c * m + df[\"votes_num\"] * df[\"rating\"]) / (c + df[\"rating\"]))\n</pre> m = df['rating'].mean() c = df[\"votes_num\"].mean() print(m) print(c) df[\"bayes_rating\"] = ((c * m + df[\"votes_num\"] * df[\"rating\"]) / (c + df[\"rating\"])) <pre>6.764515027322404\n18388.55876045464\n</pre> In\u00a0[12]: Copied! <pre>df.sort_values(\"bayes_rating\", ascending=False, inplace=True)\n</pre> df.sort_values(\"bayes_rating\", ascending=False, inplace=True) In\u00a0[13]: Copied! <pre>df = df.drop_duplicates(subset=[\"title\", \"year\", \"rating\"])\n</pre> df = df.drop_duplicates(subset=[\"title\", \"year\", \"rating\"]) In\u00a0[14]: Copied! <pre>df.head(10)\n</pre> df.head(10) Out[14]: title year certificate duration genre rating description stars votes year_start decade votes_num bayes_rating 9951 Breaking Bad (2008\u20132013) TV-MA 49.0 Crime, Drama, Thriller 9.5 A high school chemistry teacher diagnosed with... ['Bryan Cranston, ', 'Aaron Paul, ', 'Anna Gun... 1,831,359 2008 2000 1831359 952.399403 57 The Lord of the Rings: The Return of the King (2003) PG-13 201.0 Action, Adventure, Drama 9.0 Gandalf and Aragorn lead the World of Men agai... ['Peter Jackson', '| ', '    Stars:', 'Elijah ... 1,819,157 2003 2000 1819157 896.684332 9921 The Lord of the Rings: The Fellowship of the Ring (2001) PG-13 178.0 Action, Adventure, Drama 8.8 A meek Hobbit from the Shire and eight compani... ['Peter Jackson', '| ', '    Stars:', 'Elijah ... 1,844,075 2001 2000 1844075 888.836810 78 The Lord of the Rings: The Two Towers (2002) PG-13 179.0 Action, Adventure, Drama 8.8 While Frodo and Sam edge closer to Mordor with... ['Peter Jackson', '| ', '    Stars:', 'Elijah ... 1,642,708 2002 2000 1642708 792.517028 153 Gladiator (2000) R 155.0 Action, Adventure, Drama 8.5 A former Roman General sets out to exact venge... ['Ridley Scott', '| ', '    Stars:', 'Russell ... 1,481,531 2000 2000 1481531 691.273717 173 The Departed (2006) R 151.0 Crime, Drama, Thriller 8.5 An undercover cop and a mole in the police att... ['Martin Scorsese', '| ', '    Stars:', 'Leona... 1,310,171 2006 2000 1310171 612.100191 9948 Stranger Things (2016\u2013 ) TV-14 51.0 Drama, Fantasy, Horror 8.7 When a young boy disappears, his mother, a pol... ['Millie Bobby Brown, ', 'Finn Wolfhard, ', 'W... 1,149,902 2016 2010 1149902 550.545992 161 L\u00e9on: The Professional (1994) R 110.0 Action, Crime, Drama 8.5 12-year-old Mathilda is reluctantly taken in b... ['Luc Besson', '| ', '    Stars:', 'Jean Reno,... 1,147,345 1994 1990 1147345 536.869633 96 Titanic (1997) PG-13 194.0 Drama, Romance 7.9 A seventeen-year-old aristocrat falls in love ... ['James Cameron', '| ', '    Stars:', 'Leonard... 1,158,746 1997 1990 1158746 504.362454 271 Kill Bill: Vol. 1 (2003) R 111.0 Action, Crime, Drama 8.2 After awakening from a four-year coma, a forme... ['Quentin Tarantino', '| ', '    Stars:', 'Uma... 1,103,348 2003 2000 1103348 498.557567 <p>This looks better now. Let's not go further and continue with the book. This was fun.</p>"},{"location":"00_movie_analysis.html#intro-imdb-analysis","title":"Intro: IMDb Analysis\u00b6","text":""},{"location":"01_chapter/01_location_estimates.html","title":"Example: Location Estimates of Population and Murder Rates","text":"In\u00a0[1]: Copied! <pre>from scipy.stats import trim_mean\nimport pandas as pd\nimport numpy as np\nimport wquantiles\n</pre> from scipy.stats import trim_mean import pandas as pd import numpy as np import wquantiles In\u00a0[2]: Copied! <pre>state = pd.read_csv('../data/state.csv')\nstate.sort_values(\"Population\", ascending=False).head(10)\n</pre> state = pd.read_csv('../data/state.csv') state.sort_values(\"Population\", ascending=False).head(10) Out[2]: State Population Murder.Rate Abbreviation 4 California 37253956 4.4 CA 42 Texas 25145561 4.4 TX 31 New York 19378102 3.1 NY 8 Florida 18801310 5.8 FL 12 Illinois 12830632 5.3 IL 37 Pennsylvania 12702379 4.8 PA 34 Ohio 11536504 4.0 OH 21 Michigan 9883640 5.4 MI 9 Georgia 9687653 5.7 GA 32 North Carolina 9535483 5.1 NC In\u00a0[3]: Copied! <pre>trim_mean(state[\"Population\"], 0.1)\n</pre> trim_mean(state[\"Population\"], 0.1) Out[3]: <pre>np.float64(4783697.125)</pre> In\u00a0[4]: Copied! <pre>state[\"Population\"].median()\n</pre> state[\"Population\"].median() Out[4]: <pre>np.float64(4436369.5)</pre> In\u00a0[5]: Copied! <pre>np.average(state[\"Murder.Rate\"], weights=state[\"Population\"])\n</pre> np.average(state[\"Murder.Rate\"], weights=state[\"Population\"]) Out[5]: <pre>np.float64(4.445833981123393)</pre> <ul> <li>By using the weighted mean, we are actually answering the question, \u201cIf we had chosen a random residential area in the US, what would the murder rate be there?\u201d</li> <li>If we had only taken the mean of <code>state[\"Murder.Rate\"]</code>, for exapmle, we would have treated Alaska and California as regions with the same population.</li> </ul> In\u00a0[6]: Copied! <pre>wquantiles.median(state[\"Murder.Rate\"], weights=state[\"Population\"])\n</pre> wquantiles.median(state[\"Murder.Rate\"], weights=state[\"Population\"]) Out[6]: <pre>np.float64(4.4)</pre> <p>In this context, median means that half of the Americans live in states with murder rates below this value. However, since we used the <code>Population</code> feature as a weight, result is more accurate and avoids the mistake mentioned above.</p>"},{"location":"01_chapter/01_location_estimates.html#example-location-estimates-of-population-and-murder-rates","title":"Example: Location Estimates of Population and Murder Rates\u00b6","text":""},{"location":"01_chapter/02_variability_estimates.html","title":"Example: Variability Estimates of State Population","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom statsmodels import robust\n</pre> import pandas as pd from statsmodels import robust In\u00a0[2]: Copied! <pre>state = pd.read_csv('../data/state.csv')\npop = state[\"Population\"]\n</pre> state = pd.read_csv('../data/state.csv') pop = state[\"Population\"] In\u00a0[3]: Copied! <pre>pop.std()\n</pre> pop.std() Out[3]: <pre>np.float64(6848235.347401142)</pre> <p>We can also easily estimate the IQR.</p> In\u00a0[4]: Copied! <pre>pop.quantile(0.75) - pop.quantile(0.25)\n</pre> pop.quantile(0.75) - pop.quantile(0.25) Out[4]: <pre>np.float64(4847308.0)</pre> In\u00a0[5]: Copied! <pre>robust.scale.mad(pop)\n</pre> robust.scale.mad(pop) Out[5]: <pre>np.float64(3849876.1459979336)</pre>"},{"location":"01_chapter/02_variability_estimates.html#example-variability-estimates-of-state-population","title":"Example: Variability Estimates of State Population\u00b6","text":""},{"location":"01_chapter/03_exploring_data_distribution.html","title":"Exploring the Data Distribution","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre>state = pd.read_csv('../data/state.csv')\nmurder = state[\"Murder.Rate\"]\n</pre> state = pd.read_csv('../data/state.csv') murder = state[\"Murder.Rate\"] In\u00a0[3]: Copied! <pre>murder.quantile([0.05, 0.25, 0.5, 0.75, 0.95])\n</pre> murder.quantile([0.05, 0.25, 0.5, 0.75, 0.95]) Out[3]: <pre>0.05    1.600\n0.25    2.425\n0.50    4.000\n0.75    5.550\n0.95    6.510\nName: Murder.Rate, dtype: float64</pre> <p>Percentiles are valuable for summarizing the entire distribution.</p> In\u00a0[4]: Copied! <pre>ax = (state['Population']/1_000_000).plot.box()\nax.set_ylabel('Population (millions)')\n</pre> ax = (state['Population']/1_000_000).plot.box() ax.set_ylabel('Population (millions)') Out[4]: <pre>Text(0, 0.5, 'Population (millions)')</pre> <ul> <li>The green line indicates the median,</li> <li>Top and bottom of the box represent the IQR,</li> <li>Whiskers extend up to $IQR \\times 1.5$, from $Q1 - 1.5\\times IQR$ to $Q3 + 1.5\\times IQR$</li> <li>Any data outside of the whiskers is plotted as circles, often considered outliers.</li> </ul> In\u00a0[5]: Copied! <pre>binnedPop = pd.cut(state['Population'], 10)\nbinnedPop.value_counts()\n</pre> binnedPop = pd.cut(state['Population'], 10) binnedPop.value_counts() Out[5]: <pre>Population\n(526935.67, 4232659.0]      24\n(4232659.0, 7901692.0]      14\n(7901692.0, 11570725.0]      6\n(11570725.0, 15239758.0]     2\n(15239758.0, 18908791.0]     1\n(18908791.0, 22577824.0]     1\n(22577824.0, 26246857.0]     1\n(33584923.0, 37253956.0]     1\n(26246857.0, 29915890.0]     0\n(29915890.0, 33584923.0]     0\nName: count, dtype: int64</pre> In\u00a0[6]: Copied! <pre>ax = state[\"Population\"].plot.hist(figsize=(4,4))\nax.set_xlabel(\"Population\")\n</pre> ax = state[\"Population\"].plot.hist(figsize=(4,4)) ax.set_xlabel(\"Population\") Out[6]: <pre>Text(0.5, 0, 'Population')</pre> In\u00a0[7]: Copied! <pre>ax = state[\"Murder.Rate\"].plot.hist(density=True, xlim=[0,11], bins=range(0,12), figsize=[4,4])\nstate[\"Murder.Rate\"].plot.density(ax=ax, bw_method=0.3)\nax.set_xlabel(\"Murder Rate (per 100,000)\")\n</pre> ax = state[\"Murder.Rate\"].plot.hist(density=True, xlim=[0,11], bins=range(0,12), figsize=[4,4]) state[\"Murder.Rate\"].plot.density(ax=ax, bw_method=0.3) ax.set_xlabel(\"Murder Rate (per 100,000)\") Out[7]: <pre>Text(0.5, 0, 'Murder Rate (per 100,000)')</pre>"},{"location":"01_chapter/03_exploring_data_distribution.html#exploring-the-data-distribution","title":"Exploring the Data Distribution\u00b6","text":""},{"location":"01_chapter/03_exploring_data_distribution.html#percentiles-and-boxplots","title":"Percentiles and Boxplots\u00b6","text":""},{"location":"01_chapter/03_exploring_data_distribution.html#frequency-tables-and-histograms","title":"Frequency Tables and Histograms\u00b6","text":""},{"location":"01_chapter/03_exploring_data_distribution.html#density-plots-and-estimates","title":"Density Plots and Estimates\u00b6","text":""},{"location":"01_chapter/04_exploring_binary_and_categorical_data.html","title":"Exploring Binary and Categorical Data","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[2]: Copied! <pre>data = pd.read_csv(\"../data/dfw_airline.csv\")\ndata\n</pre> data = pd.read_csv(\"../data/dfw_airline.csv\") data Out[2]: Carrier ATC Weather Security Inbound 0 64263.16 84856.5 11235.42 343.15 118427.82 In\u00a0[3]: Copied! <pre>flight = data.transpose()\nflight.plot.bar(legend=False, figsize=(4,4))\n</pre> flight = data.transpose() flight.plot.bar(legend=False, figsize=(4,4)) Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre>flight_with_melt = data.melt(var_name=\"Cause\", value_name=\"Count\")\nflight_with_melt.sort_values(\"Count\", ascending=False, inplace=True)\n</pre> flight_with_melt = data.melt(var_name=\"Cause\", value_name=\"Count\") flight_with_melt.sort_values(\"Count\", ascending=False, inplace=True) In\u00a0[5]: Copied! <pre>flight_with_melt.plot.bar(x=\"Cause\", y=\"Count\", legend=True, figsize=(4,4))\n</pre> flight_with_melt.plot.bar(x=\"Cause\", y=\"Count\", legend=True, figsize=(4,4)) Out[5]: <pre>&lt;Axes: xlabel='Cause'&gt;</pre> <p>We can see that the mode of the cause of deleay for this dataset is <code>Inbound</code>.</p>"},{"location":"01_chapter/04_exploring_binary_and_categorical_data.html#exploring-binary-and-categorical-data","title":"Exploring Binary and Categorical Data\u00b6","text":""},{"location":"01_chapter/04_exploring_binary_and_categorical_data.html#further-reading","title":"Further Reading\u00b6","text":"<ul> <li>Misleading graphs</li> </ul>"},{"location":"01_chapter/05_correlation.html","title":"Correlation","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\n</pre> import pandas as pd import numpy as np import seaborn as sns In\u00a0[2]: Copied! <pre>sp500_px = pd.read_csv(\"../data/sp500_data.csv\", index_col=0, parse_dates=True)\nsp500_px\n</pre> sp500_px = pd.read_csv(\"../data/sp500_data.csv\", index_col=0, parse_dates=True) sp500_px Out[2]: ADS CA MSFT RHT CTSH CSC EMC IBM XRX ALTR ... WAT ALXN AMGN BXLT BIIB CELG GILD REGN VRTX HSIC 1993-01-29 0.000000 0.060124 -0.022100 0.000000 0.000000 0.018897 0.007368 0.092165 0.259140 -0.007105 ... 0.000000 0.000000 0.347160 0.000000 0.041670 0.000000 0.015564 1.750000 0.125000 0.000000 1993-02-01 0.000000 -0.180389 0.027621 0.000000 0.000000 0.018889 0.018425 0.115207 -0.100775 0.063893 ... 0.000000 0.000000 -0.231440 0.000000 0.000000 -0.010410 0.007782 1.250000 0.125000 0.000000 1993-02-02 0.000000 -0.120257 0.035900 0.000000 0.000000 -0.075573 0.029482 -0.023041 0.028796 -0.014192 ... 0.000000 0.000000 -0.115720 0.000000 0.000000 0.000000 -0.007792 -0.250000 0.000000 0.000000 1993-02-03 0.000000 0.060124 -0.024857 0.000000 0.000000 -0.151128 0.003689 -0.253454 -0.043190 -0.007105 ... 0.000000 0.000000 -0.086790 0.000000 0.041670 -0.041670 -0.038919 -0.500000 0.062500 0.000000 1993-02-04 0.000000 -0.360770 -0.060757 0.000000 0.000000 0.113350 -0.022114 0.069862 0.000000 -0.007096 ... 0.000000 0.000000 0.144650 0.000000 -0.041660 -0.031260 -0.046711 0.000000 0.062500 0.000000 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2015-06-25 0.019989 -0.529999 -0.379997 0.080002 0.350003 -0.580002 -0.457999 -1.720001 -0.009936 0.039997 ... -2.500000 1.889999 -2.060012 0.150002 -1.630004 -0.990005 -2.250000 1.270019 -1.919998 -0.080002 2015-06-26 -1.299988 -0.330000 -0.390004 -1.820000 0.190003 0.330002 -0.328565 -0.769989 0.020000 0.000000 ... -0.070007 -2.380005 -1.390000 0.550004 -3.509979 -1.080002 -0.800003 -1.900024 -3.629997 0.440002 2015-06-29 -10.299988 -0.410000 -0.670002 -1.250000 -1.919998 -0.510002 -0.139999 -0.949997 -0.100000 -0.040001 ... -2.250000 -5.490005 -5.029999 0.090000 -8.290009 -1.209999 -2.419998 -9.620026 -3.770004 -2.479996 2015-06-30 -2.109986 -0.049999 -0.559997 -0.099999 -0.689999 -0.610001 0.239999 -1.330001 -0.200000 0.020001 ... -0.299988 0.270004 0.279999 -1.710001 0.390014 0.439995 -0.209999 -2.070007 -0.849999 -1.360001 2015-07-01 -1.210022 0.020000 -0.009998 0.869995 0.329998 -0.190003 -0.039999 0.520004 -0.030000 -0.079998 ... -0.250000 1.270005 0.050003 -1.380001 -2.910003 1.180000 -2.099999 -8.719970 2.080001 0.940002 <p>5647 rows \u00d7 517 columns</p> In\u00a0[3]: Copied! <pre>sp500_sym = pd.read_csv(\"../data/sp500_sectors.csv\")\nsp500_sym\n</pre> sp500_sym = pd.read_csv(\"../data/sp500_sectors.csv\") sp500_sym Out[3]: sector sector_label sub_sector symbol 0 information_technology Technology data_processing_&amp;_outsourced_services ADS 1 information_technology Technology systems_software CA 2 information_technology Technology systems_software MSFT 3 information_technology Technology systems_software RHT 4 information_technology Technology it_consulting_&amp;_services CTSH ... ... ... ... ... 512 health_care Health Care biotechnology CELG 513 health_care Health Care biotechnology GILD 514 health_care Health Care biotechnology REGN 515 health_care Health Care biotechnology VRTX 516 health_care Health Care health_care_distributors HSIC <p>517 rows \u00d7 4 columns</p> In\u00a0[4]: Copied! <pre>etfs = sp500_px.loc[sp500_px.index &gt; '2012-07-01', sp500_sym[sp500_sym['sector'] == \"etf\"][\"symbol\"]]\netfs\n</pre> etfs = sp500_px.loc[sp500_px.index &gt; '2012-07-01', sp500_sym[sp500_sym['sector'] == \"etf\"][\"symbol\"]] etfs Out[4]: XLI QQQ SPY DIA GLD VXX USO IWM XLE XLY XLU XLB XTL XLV XLP XLF XLK 2012-07-02 -0.376098 0.096313 0.028223 -0.242796 0.419998 -10.400000 0.000000 0.534641 0.028186 0.095759 0.098311 -0.093713 0.019076 -0.009529 0.313499 0.018999 0.075668 2012-07-03 0.376099 0.481576 0.874936 0.728405 0.490006 -3.520000 0.250000 0.926067 0.995942 0.000000 -0.044686 0.337373 0.000000 0.000000 0.129087 0.104492 0.236462 2012-07-05 0.150440 0.096313 -0.103487 0.149420 0.239991 6.560000 -0.070000 -0.171848 -0.460387 0.306431 -0.151938 0.103086 0.019072 -0.142955 -0.073766 -0.142490 0.066211 2012-07-06 -0.141040 -0.491201 0.018819 -0.205449 -0.519989 -8.800000 -0.180000 -0.229128 0.206706 0.153214 0.080437 0.018744 -0.429213 -0.095304 0.119865 0.066495 -0.227003 2012-07-09 0.244465 -0.048160 -0.056445 -0.168094 0.429992 -0.480000 0.459999 -0.190939 -0.234892 -0.201098 -0.035751 -0.168687 0.000000 0.352630 -0.064548 0.018999 0.009457 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2015-06-25 -0.379997 -0.669998 -1.270004 -1.398679 0.010002 0.279999 -0.069999 -0.497896 -0.750000 -0.200005 -0.309997 -0.059997 -0.229999 0.189994 -0.279999 -0.220002 -0.330002 2015-06-26 -0.040001 -0.700004 -0.389999 -0.079926 0.360001 -0.020001 0.189999 -0.587512 0.259995 -0.010002 0.340000 -0.290001 -0.280002 -0.220001 0.029998 -0.030001 -0.330002 2015-06-29 -0.590000 -1.320000 -2.580002 -1.918189 0.260002 1.829999 -0.080000 -2.280327 -0.500000 -1.089996 -0.160000 -0.769996 -0.700001 -1.180000 -0.500000 -0.330000 -0.450000 2015-06-30 -0.480000 -0.419998 -1.369996 -1.168887 0.130005 0.889999 0.139999 -0.338568 -0.169998 -0.430001 -0.310001 -0.270001 0.259999 -0.510002 -0.580002 -0.150002 -0.269996 2015-07-01 -0.269996 -0.279999 -0.159989 -0.169852 -0.140000 -0.309999 -0.440001 -0.537713 -1.110001 0.220001 0.200001 -0.219997 -1.029998 0.029998 0.290001 -0.010000 -0.090000 <p>754 rows \u00d7 17 columns</p> In\u00a0[5]: Copied! <pre>sns.heatmap(etfs.corr(), vmin=-1, vmax=1)\n</pre> sns.heatmap(etfs.corr(), vmin=-1, vmax=1) Out[5]: <pre>&lt;Axes: &gt;</pre> In\u00a0[6]: Copied! <pre>telecom = sp500_px.loc[sp500_px.index &gt; \"2012-07-01\", sp500_sym[sp500_sym[\"sector\"] == \"telecommunications_services\"][\"symbol\"]]\ntelecom\n</pre> telecom = sp500_px.loc[sp500_px.index &gt; \"2012-07-01\", sp500_sym[sp500_sym[\"sector\"] == \"telecommunications_services\"][\"symbol\"]] telecom Out[6]: T CTL FTR VZ LVLT 2012-07-02 0.422496 0.140847 0.070879 0.554180 -0.519998 2012-07-03 -0.177448 0.066280 0.070879 -0.025976 -0.049999 2012-07-05 -0.160548 -0.132563 0.055128 -0.051956 -0.180000 2012-07-06 0.342205 0.132563 0.007875 0.140106 -0.359999 2012-07-09 0.136883 0.124279 -0.023626 0.253943 0.180000 ... ... ... ... ... ... 2015-06-25 0.049342 -1.600000 -0.040000 -0.187790 -0.330002 2015-06-26 -0.256586 0.039999 -0.070000 0.029650 -0.739998 2015-06-29 -0.098685 -0.559999 -0.060000 -0.504063 -1.360000 2015-06-30 -0.503298 -0.420000 -0.070000 -0.523829 0.199997 2015-07-01 -0.019737 0.080000 -0.050000 0.355811 0.139999 <p>754 rows \u00d7 5 columns</p> In\u00a0[7]: Copied! <pre>ax = telecom.plot.scatter(x='T', y='VZ', figsize=(4,4), marker=\"2\")\nax.set_xlabel(\"ATT (T)\")\nax.set_ylabel(\"Verizon (VZ)\")\nax.axhline(0, color=\"grey\", lw=1)\nax.axvline(0, color=\"grey\", lw=1)\n</pre> ax = telecom.plot.scatter(x='T', y='VZ', figsize=(4,4), marker=\"2\") ax.set_xlabel(\"ATT (T)\") ax.set_ylabel(\"Verizon (VZ)\") ax.axhline(0, color=\"grey\", lw=1) ax.axvline(0, color=\"grey\", lw=1) Out[7]: <pre>&lt;matplotlib.lines.Line2D at 0x7fa5fa322e40&gt;</pre>"},{"location":"01_chapter/05_correlation.html#correlation","title":"Correlation\u00b6","text":""},{"location":"01_chapter/05_correlation.html#scatterplots","title":"Scatterplots\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html","title":"Exploring Two or More Variables","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport seaborn as sns\n</pre> import pandas as pd import seaborn as sns In\u00a0[2]: Copied! <pre>kc_tax0 = pd.read_csv('../data/kc_tax.csv')\nkc_tax0\n</pre> kc_tax0 = pd.read_csv('../data/kc_tax.csv') kc_tax0 Out[2]: TaxAssessedValue SqFtTotLiving ZipCode 0 NaN 1730 98117.0 1 206000.0 1870 98002.0 2 303000.0 1530 98166.0 3 361000.0 2000 98108.0 4 459000.0 3150 98108.0 ... ... ... ... 498244 375000.0 2230 98056.0 498245 316000.0 1710 98056.0 498246 340000.0 1930 98056.0 498247 132000.0 2930 98056.0 498248 286000.0 1310 98056.0 <p>498249 rows \u00d7 3 columns</p> In\u00a0[3]: Copied! <pre>kc_tax0_strip = kc_tax0.loc[\n    (kc_tax0[\"TaxAssessedValue\"] &lt; 750_000) &amp;\n    (kc_tax0[\"SqFtTotLiving\"] &gt; 100) &amp;\n    (kc_tax0[\"SqFtTotLiving\"] &lt; 3500),\n    :\n]\n\nkc_tax0_strip\n</pre> kc_tax0_strip = kc_tax0.loc[     (kc_tax0[\"TaxAssessedValue\"] &lt; 750_000) &amp;     (kc_tax0[\"SqFtTotLiving\"] &gt; 100) &amp;     (kc_tax0[\"SqFtTotLiving\"] &lt; 3500),     : ]  kc_tax0_strip Out[3]: TaxAssessedValue SqFtTotLiving ZipCode 1 206000.0 1870 98002.0 2 303000.0 1530 98166.0 3 361000.0 2000 98108.0 4 459000.0 3150 98108.0 5 223000.0 1570 98032.0 ... ... ... ... 498244 375000.0 2230 98056.0 498245 316000.0 1710 98056.0 498246 340000.0 1930 98056.0 498247 132000.0 2930 98056.0 498248 286000.0 1310 98056.0 <p>432693 rows \u00d7 3 columns</p> In\u00a0[4]: Copied! <pre>ax = kc_tax0_strip.plot.hexbin(x=\"SqFtTotLiving\", y=\"TaxAssessedValue\", gridsize=30, sharex=False, figsize=(5,4))\n</pre> ax = kc_tax0_strip.plot.hexbin(x=\"SqFtTotLiving\", y=\"TaxAssessedValue\", gridsize=30, sharex=False, figsize=(5,4)) <p>As you can see, hexagonal binning plot shows the tax-assessed value, square foot and the count of records for bins at the same time.</p> <p>We take a sample from our dataset because it takes too much time to render.</p> In\u00a0[5]: Copied! <pre>sample = kc_tax0_strip.sample(n=5000, random_state=42)\n</pre> sample = kc_tax0_strip.sample(n=5000, random_state=42) In\u00a0[6]: Copied! <pre>ax = sns.kdeplot(x=sample.SqFtTotLiving, y=sample.TaxAssessedValue, fill=True)\n</pre> ax = sns.kdeplot(x=sample.SqFtTotLiving, y=sample.TaxAssessedValue, fill=True) In\u00a0[7]: Copied! <pre>lc_loans = pd.read_csv(\"../data/lc_loans.csv\")\nlc_loans\n</pre> lc_loans = pd.read_csv(\"../data/lc_loans.csv\") lc_loans Out[7]: status grade 0 Fully Paid B 1 Charged Off C 2 Fully Paid C 3 Fully Paid C 4 Current B ... ... ... 450956 Current D 450957 Current D 450958 Current D 450959 Current D 450960 Fully Paid A <p>450961 rows \u00d7 2 columns</p> In\u00a0[8]: Copied! <pre>crosstab = lc_loans.pivot_table(index=\"grade\", columns=\"status\", aggfunc=lambda x: len(x), margins=True)\ncrosstab\n</pre> crosstab = lc_loans.pivot_table(index=\"grade\", columns=\"status\", aggfunc=lambda x: len(x), margins=True) crosstab Out[8]: status Charged Off Current Fully Paid Late All grade A 1562 50051 20408 469 72490 B 5302 93852 31160 2056 132370 C 6023 88928 23147 2777 120875 D 5007 53281 13681 2308 74277 E 2842 24639 5949 1374 34804 F 1526 8444 2328 606 12904 G 409 1990 643 199 3241 All 22671 321185 97316 9789 450961 In\u00a0[9]: Copied! <pre>columns = crosstab.loc[:, \"Charged Off\":\"All\"].columns\ncrosstab[columns] = crosstab[columns].astype('float', copy=False)\n\ndf = crosstab.loc['A':'G',:].copy()\ndf.loc[:,'Charged Off':'Late'] = df.loc[:, 'Charged Off':'Late'].div(df[\"All\"], axis='index')\n\ndf['All'] = df['All'] / sum(df['All'])\nperc_crosstab = df\nperc_crosstab\n</pre> columns = crosstab.loc[:, \"Charged Off\":\"All\"].columns crosstab[columns] = crosstab[columns].astype('float', copy=False)  df = crosstab.loc['A':'G',:].copy() df.loc[:,'Charged Off':'Late'] = df.loc[:, 'Charged Off':'Late'].div(df[\"All\"], axis='index')  df['All'] = df['All'] / sum(df['All']) perc_crosstab = df perc_crosstab Out[9]: status Charged Off Current Fully Paid Late All grade A 0.021548 0.690454 0.281528 0.006470 0.160746 B 0.040054 0.709013 0.235401 0.015532 0.293529 C 0.049828 0.735702 0.191495 0.022974 0.268039 D 0.067410 0.717328 0.184189 0.031073 0.164708 E 0.081657 0.707936 0.170929 0.039478 0.077177 F 0.118258 0.654371 0.180409 0.046962 0.028614 G 0.126196 0.614008 0.198396 0.061401 0.007187 In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[10]: Copied! <pre>airline_stats = pd.read_csv(\"../data/airline_stats.csv\")\nairline_stats\n</pre> airline_stats = pd.read_csv(\"../data/airline_stats.csv\") airline_stats Out[10]: pct_carrier_delay pct_atc_delay pct_weather_delay airline 0 8.153226 1.971774 0.762097 American 1 5.959924 3.706107 1.585878 American 2 7.157270 2.706231 2.026706 American 3 12.100000 11.033333 0.000000 American 4 7.333333 3.365591 1.774194 American ... ... ... ... ... 33463 6.186422 8.798491 1.651940 Southwest 33464 9.522167 3.591133 0.261084 Southwest 33465 9.164179 2.664179 0.343284 Southwest 33466 5.152293 1.964520 0.122817 Southwest 33467 3.964393 1.700479 0.019449 Southwest <p>33468 rows \u00d7 4 columns</p> In\u00a0[11]: Copied! <pre>ax = airline_stats.boxplot(by=\"airline\", column=\"pct_carrier_delay\")\n</pre> ax = airline_stats.boxplot(by=\"airline\", column=\"pct_carrier_delay\") <p>As informative as boxplots are, using a different type of plot here, which is violin plot, eliminates the ugly appearance caused by outliers.</p> In\u00a0[12]: Copied! <pre>ax = sns.violinplot(x=airline_stats.airline, y=airline_stats.pct_carrier_delay, inner=\"quartile\", color=\"white\")\n_ =  ax.set_ylabel(\"% of daily airline delays by carrier\")\n</pre> ax = sns.violinplot(x=airline_stats.airline, y=airline_stats.pct_carrier_delay, inner=\"quartile\", color=\"white\") _ =  ax.set_ylabel(\"% of daily airline delays by carrier\") In\u00a0[13]: Copied! <pre>zip_codes = [98188, 98105, 98108, 98126]\nkc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes), :]\nkc_tax_zip\n</pre> zip_codes = [98188, 98105, 98108, 98126] kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes), :] kc_tax_zip Out[13]: TaxAssessedValue SqFtTotLiving ZipCode 3 361000.0 2000 98108.0 4 459000.0 3150 98108.0 10 202000.0 830 98108.0 11 210000.0 1130 98108.0 12 193000.0 1560 98108.0 ... ... ... ... 498049 346000.0 1430 98105.0 498050 463000.0 1610 98105.0 498051 553000.0 1580 98105.0 498052 571000.0 1840 98105.0 498053 694000.0 2420 98105.0 <p>22038 rows \u00d7 3 columns</p> In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef hexbin(x,y,color,**kwargs):\n    cmap = sns.light_palette(color,as_cmap=True)\n    plt.hexbin(x,y,gridsize=25, cmap=cmap, **kwargs)\n\ng = sns.FacetGrid(kc_tax_zip, col=\"ZipCode\", col_wrap=2)\ng.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', extent=[0,3500,0,700000])\ng.set_axis_labels(\"Finished Square Feet\", \"Tax-Assessed Value\")\ng.set_titles(\"Zip code {col_name:.0f}\")\n</pre> import matplotlib.pyplot as plt  def hexbin(x,y,color,**kwargs):     cmap = sns.light_palette(color,as_cmap=True)     plt.hexbin(x,y,gridsize=25, cmap=cmap, **kwargs)  g = sns.FacetGrid(kc_tax_zip, col=\"ZipCode\", col_wrap=2) g.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', extent=[0,3500,0,700000]) g.set_axis_labels(\"Finished Square Feet\", \"Tax-Assessed Value\") g.set_titles(\"Zip code {col_name:.0f}\") Out[14]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7ff7b5b64d70&gt;</pre>"},{"location":"01_chapter/06_exploring_two_or_more_variables.html#exploring-two-or-more-variables","title":"Exploring Two or More Variables\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html#hexagonal-binning-and-contours-plotting-numeric-versus-numeric-data","title":"Hexagonal Binning and Contours (Plotting Numeric Versus Numeric Data)\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html#contours","title":"Contours\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html#contingency-tables","title":"Contingency Tables\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html#categorical-and-numeric-data","title":"Categorical and Numeric Data\u00b6","text":""},{"location":"01_chapter/06_exploring_two_or_more_variables.html#visualizing-mulitple-variables","title":"Visualizing Mulitple Variables\u00b6","text":""},{"location":"02_chapter/01_sampling_distribution_of_statistic.html","title":"Sampling Distribution of a Statistic","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport seaborn as sns\n</pre> import pandas as pd import seaborn as sns In\u00a0[2]: Copied! <pre>data = pd.read_csv(\"../data/loans_income.csv\")\ndata\n</pre> data = pd.read_csv(\"../data/loans_income.csv\") data Out[2]: x 0 67000 1 52000 2 100000 3 78762 4 37041 ... ... 49995 40000 49996 54000 49997 50000 49998 82000 49999 70000 <p>50000 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre>sample_data = pd.DataFrame({\n    'income': data[\"x\"].sample(1000),\n    'type': \"Data\"\n})\nsample_data\n</pre> sample_data = pd.DataFrame({     'income': data[\"x\"].sample(1000),     'type': \"Data\" }) sample_data  Out[3]: income type 34623 45000 Data 23129 53000 Data 26682 51000 Data 564 55000 Data 27425 57798 Data ... ... ... 16981 121000 Data 33186 37500 Data 42905 40000 Data 107 64000 Data 8903 154000 Data <p>1000 rows \u00d7 2 columns</p> In\u00a0[4]: Copied! <pre>sample_mean_5 = pd.DataFrame({\n    'income': [data[\"x\"].sample(5).mean() for _ in range(1000)],\n    'type': \"Mean of 5\"\n})\n\nsample_mean_20 = pd.DataFrame({\n    'income': [data[\"x\"].sample(20).mean() for _ in range(1000)],\n    'type': \"Mean of 20\"\n})\n\nsamples = pd.concat([sample_data, sample_mean_5, sample_mean_20])\nsamples\n</pre> sample_mean_5 = pd.DataFrame({     'income': [data[\"x\"].sample(5).mean() for _ in range(1000)],     'type': \"Mean of 5\" })  sample_mean_20 = pd.DataFrame({     'income': [data[\"x\"].sample(20).mean() for _ in range(1000)],     'type': \"Mean of 20\" })  samples = pd.concat([sample_data, sample_mean_5, sample_mean_20]) samples Out[4]: income type 34623 45000.00 Data 23129 53000.00 Data 26682 51000.00 Data 564 55000.00 Data 27425 57798.00 Data ... ... ... 995 72136.65 Mean of 20 996 70170.00 Mean of 20 997 68407.75 Mean of 20 998 73184.40 Mean of 20 999 76592.80 Mean of 20 <p>3000 rows \u00d7 2 columns</p> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\n\ng = sns.FacetGrid(samples, col=\"type\")\ng.map(plt.hist, 'income', bins=40)\n</pre> import matplotlib.pyplot as plt  g = sns.FacetGrid(samples, col=\"type\") g.map(plt.hist, 'income', bins=40) Out[5]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7f4a7c5db770&gt;</pre>"},{"location":"02_chapter/01_sampling_distribution_of_statistic.html#sampling-distribution-of-a-statistic","title":"Sampling Distribution of a Statistic\u00b6","text":""},{"location":"02_chapter/02_bootstrap.html","title":"The Bootstrap","text":"In\u00a0[1]: Copied! <pre>from sklearn.utils import resample\nimport pandas as pd\n</pre> from sklearn.utils import resample import pandas as pd In\u00a0[2]: Copied! <pre>loans_income = pd.read_csv(\"../data/loans_income.csv\")\nloans_income\n</pre> loans_income = pd.read_csv(\"../data/loans_income.csv\") loans_income Out[2]: x 0 67000 1 52000 2 100000 3 78762 4 37041 ... ... 49995 40000 49996 54000 49997 50000 49998 82000 49999 70000 <p>50000 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre>results = []\nfor _ in range(1000):\n    sample = resample(loans_income)\n    results.append(sample[\"x\"].mean())\n\nres_df = pd.Series(results)\nres_df\n</pre> results = [] for _ in range(1000):     sample = resample(loans_income)     results.append(sample[\"x\"].mean())  res_df = pd.Series(results) res_df Out[3]: <pre>0      68819.51108\n1      68619.92176\n2      68720.51408\n3      68902.93970\n4      68766.86440\n          ...     \n995    68681.72210\n996    68688.25678\n997    68723.05218\n998    68877.61952\n999    68770.97882\nLength: 1000, dtype: float64</pre> In\u00a0[4]: Copied! <pre>print(f\"Original median: {loans_income[\"x\"].mean()}\")\nprint(f\"Bootstrap method median {res_df.mean()}\")\nprint(f\"Bias: {res_df.mean()- loans_income[\"x\"].mean()}\")\nprint(f\"Std. Error: {res_df.std()}\")\n</pre> print(f\"Original median: {loans_income[\"x\"].mean()}\") print(f\"Bootstrap method median {res_df.mean()}\") print(f\"Bias: {res_df.mean()- loans_income[\"x\"].mean()}\") print(f\"Std. Error: {res_df.std()}\") <pre>Original median: 68760.51844\nBootstrap method median 68759.56455244\nBias: -0.9538875599973835\nStd. Error: 145.5321894680058\n</pre>"},{"location":"02_chapter/02_bootstrap.html#the-bootstrap","title":"The Bootstrap\u00b6","text":""},{"location":"02_chapter/03_normal_distribution.html","title":"Normal Distribution","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom scipy.stats import probplot, norm\nimport pandas as pd\n\nfig, ax = plt.subplots(figsize=(4,4))\nnorm_sample = norm.rvs(size=100)\nprobplot(norm_sample, plot=ax)\n</pre> import matplotlib.pyplot as plt from scipy.stats import probplot, norm import pandas as pd  fig, ax = plt.subplots(figsize=(4,4)) norm_sample = norm.rvs(size=100) probplot(norm_sample, plot=ax) Out[1]: <pre>((array([-2.46203784, -2.12570747, -1.93122778, -1.79044653, -1.67819304,\n         -1.58381122, -1.50174123, -1.42869743, -1.36256869, -1.30191411,\n         -1.24570419, -1.19317644, -1.14374949, -1.09696931, -1.05247413,\n         -1.00997067, -0.96921765, -0.93001393, -0.89218993, -0.85560121,\n         -0.82012357, -0.78564937, -0.75208458, -0.71934648, -0.68736185,\n         -0.65606548, -0.62539893, -0.59530962, -0.56574992, -0.53667655,\n         -0.50804994, -0.47983378, -0.45199463, -0.42450149, -0.39732558,\n         -0.37044003, -0.34381966, -0.31744076, -0.29128096, -0.26531902,\n         -0.23953472, -0.21390872, -0.18842244, -0.16305799, -0.13779803,\n         -0.1126257 , -0.08752455, -0.06247843, -0.03747145, -0.01248789,\n          0.01248789,  0.03747145,  0.06247843,  0.08752455,  0.1126257 ,\n          0.13779803,  0.16305799,  0.18842244,  0.21390872,  0.23953472,\n          0.26531902,  0.29128096,  0.31744076,  0.34381966,  0.37044003,\n          0.39732558,  0.42450149,  0.45199463,  0.47983378,  0.50804994,\n          0.53667655,  0.56574992,  0.59530962,  0.62539893,  0.65606548,\n          0.68736185,  0.71934648,  0.75208458,  0.78564937,  0.82012357,\n          0.85560121,  0.89218993,  0.93001393,  0.96921765,  1.00997067,\n          1.05247413,  1.09696931,  1.14374949,  1.19317644,  1.24570419,\n          1.30191411,  1.36256869,  1.42869743,  1.50174123,  1.58381122,\n          1.67819304,  1.79044653,  1.93122778,  2.12570747,  2.46203784]),\n  array([-2.87940732, -2.38034489, -2.24503669, -1.87523997, -1.57398552,\n         -1.53752261, -1.49874984, -1.46552663, -1.37490677, -1.35934299,\n         -1.27238499, -1.26333362, -1.24922741, -1.20444412, -1.11544598,\n         -1.07546264, -1.06828847, -1.04793949, -1.00854077, -0.96733759,\n         -0.91162785, -0.86130556, -0.8198231 , -0.69995537, -0.64501357,\n         -0.63453225, -0.62952914, -0.59505563, -0.57294289, -0.49703109,\n         -0.48328752, -0.46758467, -0.45469407, -0.40142057, -0.38175195,\n         -0.36560056, -0.34673531, -0.32501623, -0.23260236, -0.20341978,\n         -0.17258112, -0.07115338, -0.05349291, -0.02440474, -0.00777053,\n          0.01942193,  0.07743442,  0.10133061,  0.13994562,  0.21343922,\n          0.21633112,  0.23704499,  0.26424302,  0.28936311,  0.31425113,\n          0.34684109,  0.35563148,  0.36027773,  0.3886194 ,  0.40637999,\n          0.41361807,  0.4554674 ,  0.45712821,  0.4595022 ,  0.48068486,\n          0.49778227,  0.50006648,  0.55360245,  0.55665191,  0.59511136,\n          0.60835404,  0.61341771,  0.67476374,  0.68691344,  0.71587014,\n          0.8752446 ,  0.92669015,  0.96072594,  1.01667322,  1.02154404,\n          1.05898798,  1.09645091,  1.12023027,  1.13749697,  1.14178507,\n          1.14194743,  1.1545707 ,  1.22597444,  1.22992192,  1.25845365,\n          1.26010246,  1.29511115,  1.56339963,  1.6417792 ,  1.64841813,\n          1.65376612,  1.66387724,  1.90514103,  1.9245629 ,  2.07488121])),\n (np.float64(1.0459725988214466),\n  np.float64(0.04676425017249783),\n  np.float64(0.9930523474795839)))</pre> In\u00a0[2]: Copied! <pre>loans = pd.read_csv(\"../data/loans_income.csv\")\nloan_values = loans[\"x\"].dropna()\nz_loan = (loan_values - loan_values.mean()) / loan_values.std()\nz_loan\n</pre> loans = pd.read_csv(\"../data/loans_income.csv\") loan_values = loans[\"x\"].dropna() z_loan = (loan_values - loan_values.mean()) / loan_values.std() z_loan Out[2]: <pre>0       -0.053557\n1       -0.509872\n2        0.950336\n3        0.304255\n4       -0.964939\n           ...   \n49995   -0.874924\n49996   -0.449030\n49997   -0.570714\n49998    0.402758\n49999    0.037706\nName: x, Length: 50000, dtype: float64</pre> In\u00a0[3]: Copied! <pre>fig, ax = plt.subplots(figsize=(4,4))\nprobplot(z_loan, plot=ax)\n</pre> fig, ax = plt.subplots(figsize=(4,4)) probplot(z_loan, plot=ax) Out[3]: <pre>((array([-4.19138481, -3.98563638, -3.8734641 , ...,  3.8734641 ,\n          3.98563638,  4.19138481], shape=(50000,)),\n  array([-1.97007936, -1.88185848, -1.87881638, ...,  3.94452245,\n          3.96201452,  3.96201452], shape=(50000,))),\n (np.float64(0.9658789452608367),\n  np.float64(-4.2414428944933633e-16),\n  np.float64(0.9658085995476677)))</pre>"},{"location":"02_chapter/03_normal_distribution.html#normal-distribution","title":"Normal Distribution\u00b6","text":""},{"location":"02_chapter/03_normal_distribution.html#standard-normal-and-qq-plots","title":"Standard Normal and QQ-Plots\u00b6","text":""}]}